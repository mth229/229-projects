{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Newton's method\n\n\nWe get started by loading our package that brings in plotting and other features, including those provided by the `Roots` package:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using MTH229\nusing Plots"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n\n### Quick background\n\nRead about this material here: [Newton's Method](http://mth229.github.io/newton.html).\n\nFor the impatient, symbolic math - as is done behind the scenes at\nthe Wolfram alpha web site - is pretty nice. For so many problems it\ncan easily do what is tedious work. However, for some questions, only\nnumeric solutions are possible. For example, there is no general\nformula to solve a fifth order polynomial the way there is a quadratic\nformula for solving quadratic polynomials. Even an innocuous\npolynomial like $f(x) = x^5 - x - 1$ has no easy algebraic solution for is one root.\n\nA graph shows what looks like just one answer between $1$ and $2$, closer to $1$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = x^5 - x - 1\nplot(f, -2, 2)\nplot!(zero, -2, 2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this, we can call the bisection method to identify the value:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "fzero(f, -2, 2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've seen the bisection method previously to find a root, but this\nis somewhat cumbersome to use as it needs a\n[bracketing](https://en.wikipedia.org/wiki/Bisection_method#The_method)\ninterval to begin. Moreover, bisection can be computationally slow (by comparison).\n\nHere we discuss Newton's method. Like the bisection method it is an\n*iterative algorithm*. However instead of identifying a bracketing\ninterval, we only need to identify a reasonable *initial* guess,\n$x_0$.\n\n\nStarting with $x_0$ the algorithm to produce $x_1$ is easy to describe:\n\n* form the tangent line at $(x_0, f(x_0))$.\n\n* let $x_1$ be the intersection point of this tangent line with the $x$ axis.\n\nIf we can go from $x_0$ to $x_1$ we can repeat the update step to get $x_2$ and then $x_3$, ...\n\nGraphically, this figure illustrates the process:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = x^5 - x - 1\nnewton_vis(f, 1, 0.95, 1.3)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the figure, the sequence of guesses can be seen, basically $1$, $1.25$, $1.178\\dots$, $1.167\\dots$, ...\n\nTo find these numerically, we first need an algebraic\nrepresentation. For this problem, we can describe the tangent line's\nslope by *either* $f'(x_0)$ *or* by using \"rise over run\":\n\n$$~\nf'(x_0) = \\frac{f(x_0)-f(x_1)}{x_0 - x_1}\n~$$\n\nUsing $f(x_1)=0$, this yields the update formula: $x_1 = x_0 - f(x_0)\n/ f'(x_0)$. That is, the new guess shifts the old guess by an increment\n$f(x_0)/f'(x_0)$.\n\n\nIn `Julia`, we can do one step with:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = x^5 - x - 1\nx = 1\nx = x - f(x) / f'(x)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "(We don't use indexing, but rather update our binding for the `x` variable.)\n\nIs `x` close to being the zero? We don't know the actual zero - we\nare trying to approximate it - but we do know the function's value at\nthe actual zero. For this new guess the function value is"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is much closer to $0$ than $f(1)$, the value at our initial guess, but\nnot nearly as close as we can get using Newton's method. We just need to **iterate** -\nrun a few more steps.\n\nWe do another step just by running the last line. For example, we run 5 more steps by copying and pasting the same expression:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x = x - f(x) / f'(x)\nx = x - f(x) / f'(x)\nx = x - f(x) / f'(x)\nx = x - f(x) / f'(x)\nx = x - f(x) / f'(x)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The value of `x` updates. But is it getting closer to a *zero*? If so,\nthen $f(x)$ should be close to zero. We can see both values with:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x, f(x)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows $f(x)$ is not exactly $0.0$ but it is as close as we can\nget. Repeating the algorithm does not change the value of `x`. (On a\ncomputer, floating point issues creep in when values are close to 0,\nand these prevent values being mathematically exact.) As we can't\nimprove, we stop. Our value of `x` is an *approximate* zero and\n`f(x)` is within machine tolerance of being `0`.\n\n\nHow do we know how to stop? When the algorithm works, we will stop\nwhen the `x` value *basically* stops updating, as `f(x)` is basically\n`0`. However, the algorithm need not work, so any implementation must\nkeep track of how many steps are taken and stop when this gets out of\nhand.\n\n\nFor convenience, the `newton` function in  the `MTH229` package (using the `Roots` package) will\niterate until convergence. If we pass in the optional argument\n`verbose=true` we will see the sequence of steps.\n\nFor example, for $f(x) = x^3 - 2x - 5$, a function that Newton\nhimself considered, a solution near $2$, is found with:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x = 2\nf(x) = x^3 - 2x -5\nxstar = newton(f, 2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the approximate zero and the function value, as follows:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "xstar, f(xstar)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n\n\n\n### Using fzero from the Roots package\n\nAs mentioned, the `newton` function in the `Roots` package implements\nNewton's method.  The `Roots` package also provides the `fzero`\nfunction for finding roots. (also known as `find_zero`.)  We have seen it used with a bracketing\ninterval, but it also provides a solution when just given an initial\nguess - like Newton's method:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "fzero(sin, 3)   # start with initial guess of 3, returns 3.141592653589793"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The utility of this function is that it does not require a derivative to be\ntaken and it is a little less sensitive than Newton's method to the\ninitial guess. The use of `fzero` is recommended.\n\n\n### When Newton's method fails\n\n\nThe error in the $n$th step using Newton's method at a simple zero follows a formula:\n$|e_{n+1}| \\leq (1/2) |f''(a)/f'(b)| \\cdot |e_n|^2$,\nfor some $a$ and $b$. Generally this ensures that the error at step\n$n+1$ is smaller than the error at step $n$ squared. But this can fail due to\nvarious cases:\n\n* the initial guess is not close to the zero\n\n* the derivative, $|f'(x)|$, is too small\n\n* the second derivative, $|f''(x)|$, is too big, or possibly undefined.\n\n\n\n### Quadratic convergence\n\nWhen Newton's method converges to a *simple zero* it is said to have\n*quadratic convergence*. A simple zero is one with multiplicity 1 and\nquadratic convergence says basically that the error at the $i+1$st\nstep is like the error for $i$th step squared. In particular, if the\nerror is like $10^{-3}$ on one step, it will be like $10^{-6}$, then\n$10^{-12}$ then $10^{-24}$ on subsequent steps. (Which is typically\nbeyond the limit of a floating point approximation.) This is why one\ncan *usually* take just 5, or so, steps to get to an answer.\n\nNot so for multiple roots and some simple roots.\n\n\n----"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.7.0"
    },
    "kernelspec": {
      "name": "julia-1.7",
      "display_name": "Julia 1.7.0",
      "language": "julia"
    }
  },
  "nbformat": 4
}
