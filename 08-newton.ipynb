{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Newton's method\n\n\nWe get started by loading our package that brings in plotting and other features, including those provided by the `Roots` package:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using MTH229\nusing Plots\nplotly()"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n\n### Quick background\n\nRead about this material here: [Newton's Method](http://mth229.github.io/newton.html).\n\nFor the impatient, symbolic math - as is done behind the scenes at\nthe Wolfram alpha web site - is pretty nice. For so many problems it\ncan easily do what is tedious work. However, for some questions, only\nnumeric solutions are possible. For example, there is no general\nformula to solve a fifth order polynomial the way there is a quadratic\nformula for solving quadratic polynomials. Even an innocuous\npolynomial like ``f(x) = x^5 - x - 1`` has no easy algebraic solution for is one root.\n\nA graph shows what looks like just one answer between ``1`` and ``2``, closer to ``1``"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = x^5 - x - 1\nplot(f, -2, 2)\nplot!(zero, -2, 2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this, we can call the bisection method to identify the value:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "fzero(f, -2, 2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've seen the bisection method previously to find a root, but this\nis somewhat cumbersome to use as it needs a\n[bracketing](https://en.wikipedia.org/wiki/Bisection_method#The_method)\ninterval to begin. Moreover, bisection can be computationally slow (by comparison).\n\nHere we discuss Newton's method. Like the bisection method it is an\n*iterative algorithm*. However instead of identifying a bracketing\ninterval, we only need to identify a reasonable *initial* guess,\n``x_0``.\n\n\nStarting with ``x_0`` the algorithm to produce ``x_1`` is easy to describe:\n\n* form the tangent line at ``(x_0, f(x_0))``.\n\n* let ``x_1`` be the intersection point of this tangent line with the ``x`` axis.\n\nIf we can go from ``x_0`` to ``x_1`` we can repeat the update step to get ``x_2`` and then ``x_3``, ...\n\nGraphically, this figure illustrates the process:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = x^5 - x - 1\nnewton_vis(f, 1, 0.95, 1.3)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the figure, the sequence of guesses can be seen, basically ``1``, ``1.25``, ``1.178\\dots``, ``1.167\\dots``, ...\n\nTo find these numerically, we first need an algebraic\nrepresentation. For this problem, we can describe the tangent line's\nslope by *either* ``f'(x_0)`` *or* by using \"rise over run\":\n\n```math\nf'(x_0) = \\frac{f(x_0) - 0}{x_0 - x_1}\n```\n\nSolving for ``x_0``, this yields the update formula:\n\n```math\nx_1 = x_0 - f(x_0) / f'(x_0).\n```\n\nThat is, the new guess shifts the old guess by an increment\n``f(x_0)/f'(x_0)``.\n\n\nIn `Julia`, we can do one step with:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = x^5 - x - 1\nx = 1\nx = x - f(x) / f'(x)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "(We don't use indexing, but rather update our binding for the `x` variable.)\n\nIs `x` close to being the zero? We don't know the actual zero - we\nare trying to approximate it - but we do know the function's value at\nthe actual zero. For this new guess the function value is"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is much closer to ``0`` than ``f(1)``, the value at our initial guess, but\nnot nearly as close as we can get using Newton's method. We just need to **iterate** -\nrun a few more steps.\n\nWe do another step just by running the last line. For example, we run 5 more steps by copying and pasting the same expression:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x = x - f(x) / f'(x)\nx = x - f(x) / f'(x)\nx = x - f(x) / f'(x)\nx = x - f(x) / f'(x)\nx = x - f(x) / f'(x)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The value of `x` updates. But is it getting closer to a *zero*? If so,\nthen ``f(x)`` should be close to zero. We can see both values with:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x, f(x)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows ``f(x)`` is not exactly ``0.0`` but it is as close as we can\nget. Repeating the algorithm does not change the value of `x`. (On a\ncomputer, floating point issues creep in when values are close to 0,\nand these prevent values being mathematically exact.) As we can't\nimprove, we stop. Our value of `x` is an *approximate* zero and\n`f(x)` is within machine tolerance of being `0`.\n\n\nHow do we know how to stop? When the algorithm works, we will stop\nwhen the `x` value *basically* stops updating, as `f(x)` is basically\n`0`. However, the algorithm need not work, so any implementation must\nkeep track of how many steps are taken and stop when this gets out of\nhand.\n\n\nFor convenience, the `newton` function in  the `MTH229` package (using the `Roots` package) will\niterate until convergence. If we pass in the optional argument\n`verbose=true` we will see the sequence of steps.\n\nFor example, for ``f(x) = x^3 - 2x - 5``, a function that Newton\nhimself considered, a solution near ``2``, is found with:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x = 2\nf(x) = x^3 - 2x -5\nxstar = newton(f, 2)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the approximate zero and the function value, as follows:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "xstar, f(xstar)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n\n\n\n### Using fzero from the Roots package\n\nAs mentioned, the `newton` function in the `Roots` package implements\nNewton's method.  The `Roots` package also provides the `fzero`\nfunction for finding roots. (also known as `find_zero`.)  We have seen it used with a bracketing\ninterval, but it also provides a solution when just given an initial\nguess - like Newton's method:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "fzero(sin, 3)   # start with initial guess of 3, returns 3.141592653589793"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The utility of this function is that it does not require a derivative to be\ntaken and it is a little less sensitive than Newton's method to the\ninitial guess. The use of `fzero` is recommended.\n\n\n### When Newton's method fails\n\n\nThe error in the ``n``th step using Newton's method at a simple zero follows a formula:\n``|e_{n+1}| \\leq (1/2) |f''(a)/f'(b)| \\cdot |e_n|^2``,\nfor some ``a`` and ``b``. Generally this ensures that the error at step\n``n+1`` is smaller than the error at step ``n`` squared. But this can fail due to\nvarious cases:\n\n* the initial guess is not close to the zero\n\n* the derivative, ``|f'(x)|``, is too small\n\n* the second derivative, ``|f''(x)|``, is too big, or possibly undefined.\n\n\n\n### Quadratic convergence\n\nWhen Newton's method converges to a *simple zero* it is said to have\n*quadratic convergence*. A simple zero is one with multiplicity 1 and\nquadratic convergence says basically that the error at the ``i+1``st\nstep is like the error for ``i``th step squared. In particular, if the\nerror is like ``10^{-3}`` on one step, it will be like ``10^{-6}``, then\n``10^{-12}`` then ``10^{-24}`` on subsequent steps. (Which is typically\nbeyond the limit of a floating point approximation.) This is why one\ncan *usually* take just 5, or so, steps to get to an answer.\n\nNot so for multiple roots and some simple roots.\n\n\n----"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.7.3"
    },
    "kernelspec": {
      "name": "julia-1.7",
      "display_name": "Julia 1.7.3",
      "language": "julia"
    }
  },
  "nbformat": 4
}
